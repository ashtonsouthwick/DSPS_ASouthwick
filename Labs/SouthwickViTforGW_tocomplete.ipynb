{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashtonsouthwick/DSPS_ASouthwick/blob/main/Labs/SouthwickViTforGW_tocomplete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "import pandas as pd\n",
        "from skimage import io\n",
        "from skimage.color import rgb2gray\n",
        "from skimage.transform import rescale\n",
        "import numpy as np\n",
        "from functools import reduce\n",
        "import pylab as plt\n",
        "\n",
        "DIRECTLYFROMGRAVITYSPY = False"
      ],
      "metadata": {
        "id": "FSatv31GbrIT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "an-8nvV2bmRn"
      },
      "outputs": [],
      "source": [
        "\n",
        "def read_and_crop_image(filename, x, y):\n",
        "    \"\"\"Read in a crop part of image you want to keep\n",
        "\n",
        "    Parameters\n",
        "        filename (str):\n",
        "            the file you would like to pixelize\n",
        "\n",
        "        x (float, list):\n",
        "            xrange of pixels to keep\n",
        "\n",
        "        y (float, list):\n",
        "            yrange of pixels to keep\n",
        "\n",
        "\n",
        "    Returns\n",
        "        image_data (`np.array):\n",
        "            this images is taken from rgb to gray scale\n",
        "            and then downsampled by the resolution.\n",
        "    \"\"\"\n",
        "    xmin = x[0]\n",
        "    xmax = x[1]\n",
        "    ymin = y[0]\n",
        "    ymax = y[1]\n",
        "    image_data = io.imread(filename)\n",
        "    image_data = image_data[xmin:xmax, ymin:ymax, :3]\n",
        "    return image_data\n",
        "\n",
        "\n",
        "def read_grayscale(filename, resolution=0.3, x=[66, 532], y=[105, 671],\n",
        "                   verbose=False):\n",
        "    \"\"\"Convert image from RGB to Gray, downsample\n",
        "\n",
        "    Parameters\n",
        "        filename (str):\n",
        "            the file you would like to pixelize\n",
        "\n",
        "        resolution (float, optional):\n",
        "            default: 0.3\n",
        "\n",
        "        verbose (bool, optional):\n",
        "            default: False\n",
        "\n",
        "    Returns\n",
        "        image_data (`np.array):\n",
        "            this images is taken from rgb to gray scale\n",
        "            and then downsampled by the resolution.\n",
        "    \"\"\"\n",
        "    image_data = read_and_crop_image(filename, x=x, y=y)\n",
        "\n",
        "    image_data = rgb2gray(image_data)\n",
        "    image_data = rescale(image_data, resolution, mode='constant',\n",
        "                         preserve_range='True', channel_axis=None)\n",
        "\n",
        "    #dim = int(reduce(lambda x, y: x * y, image_data.shape))\n",
        "    #image_data = np.reshape(image_data, (dim))\n",
        "    image_data = np.array(image_data, dtype='f')\n",
        "\n",
        "    return image_data\n",
        "\n",
        "\n",
        "def read_rgb(filename, resolution=0.3, x=[66, 532], y=[105, 671],\n",
        "             verbose=False):\n",
        "    \"\"\"Convert image from RGB to Gray, downsample\n",
        "\n",
        "    Parameters\n",
        "        filename (str):\n",
        "            the file you would like to pixelize\n",
        "\n",
        "        resolution (float, optional):\n",
        "            default: 0.3\n",
        "\n",
        "        verbose (bool, optional):\n",
        "            default: False\n",
        "\n",
        "    Returns\n",
        "        image_data (`np.array):\n",
        "            this images is taken from rgb to gray scale\n",
        "            and then downsampled by the resolution.\n",
        "    \"\"\"\n",
        "    image_data = read_and_crop_image(filename, x=x, y=y)\n",
        "    image_data = rescale(image_data, resolution, mode='constant',\n",
        "                         preserve_range='True', channel_axis=-1)\n",
        "    dim = int(reduce(lambda x, y: x * y, image_data[:, :, 0].shape))\n",
        "    image_data_r = np.reshape(image_data[:, :, 0], (dim))\n",
        "    image_data_g = np.reshape(image_data[:, :, 1], (dim))\n",
        "    image_data_b = np.reshape(image_data[:, :, 2], (dim))\n",
        "\n",
        "    return image_data_r, image_data_g, image_data_b"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "run the cell below to select your classes and read in teh images directly from GravitySpy"
      ],
      "metadata": {
        "id": "mGnLhVxAlXQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if DIRECTLYFROMGRAVITYSPY:\n",
        "  labels = []\n",
        "  imgs = np.zeros((2264, 140, 170))\n",
        "  j, k = 0, 0\n",
        "  for i,f in enumerate(glob(\"H1L1/Chirp/*\")):\n",
        "      #print(f)\n",
        "      if k == 264:\n",
        "          break\n",
        "      img = read_grayscale(f)\n",
        "      if i%100 == 0 :\n",
        "          plt.imshow(img)\n",
        "          plt.title(f\"Chirp {i} \")\n",
        "          plt.show()\n",
        "      imgs[j] = img\n",
        "      labels.append([\"chirp\"])\n",
        "      k += 1\n",
        "      j += 1\n",
        "  print(i)\n",
        "  k = 0\n",
        "  for i,f in enumerate(glob(\"H1L1/Low_Frequency_Burst/*\")) :\n",
        "      if k == 1000:\n",
        "          break\n",
        "      img = read_grayscale(f)\n",
        "      if i%100 == 0 :\n",
        "          plt.imshow(img)\n",
        "          plt.title(f\"Low_Frequency_Burst {i}\")\n",
        "          plt.show()\n",
        "\n",
        "      imgs[j] = img\n",
        "      labels.append([\"low_frequency_burst\"])\n",
        "      k += 1\n",
        "      j += 1\n",
        "  print(i,j,k)\n",
        "\n",
        "  k = 0\n",
        "  for i,f in enumerate(glob(\"H1L1/Koi_Fish/*\")) :\n",
        "      if k == 1000:\n",
        "          break\n",
        "      img = read_grayscale(f)\n",
        "      if i%100 == 0 :\n",
        "          plt.imshow(img)\n",
        "          plt.title(f\"koi_fish {i}\")\n",
        "          plt.show()\n",
        "\n",
        "      imgs[j] = img\n",
        "      labels.append([\"koi_fish\"])\n",
        "      k += 1\n",
        "      j += 1\n"
      ],
      "metadata": {
        "id": "ALKdI5yAbnql"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "run the two cells below to read the pre-made array of data"
      ],
      "metadata": {
        "id": "pYgRa--LlhDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not DIRECTLYFROMGRAVITYSPY:\n",
        "  labels = pd.read_csv(\"gw_labels.csv\", index_col=0)\n",
        "  labels"
      ],
      "metadata": {
        "id": "4UxyUN4UccgR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not DIRECTLYFROMGRAVITYSPY:\n",
        "  imgs = np.load(open(\"gw_imgs.npy\", \"rb\"))\n",
        "  imgs.shape\n",
        "#reshaping images with a new axis because vision transformers are commonly built for 3 axis images\n",
        "imgs = imgs[:,:,:,np.newaxis]\n",
        "imgs.shape"
      ],
      "metadata": {
        "id": "S2DYh662kMP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#show a few images\n",
        "np.random.seed(302)\n",
        "for i in np.random.choice(imgs.shape[0], 10):\n",
        "  plt.imshow(imgs[i])\n",
        "  plt.title(labels.iloc[i])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "avFLhJeullpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhWN56lxoKj9",
        "outputId": "cdc1c221-2eeb-47db-ca29-b8b631b55bb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['chirp', 'low_frequency_burst', 'koi_fish'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "\n",
        "# Encode string labels into numerical format\n",
        "#label_encoder = LabelEncoder()\n",
        "#encoded_labels_sparse = label_encoder.fit_transform(labels)\n",
        "#sparse encoder makes a 1D label with numbers 0-n, use sparse_categorical_crossentropy loss\n",
        "\n",
        "# Convert to one-hot encoded format\n",
        "...\n",
        "...\n",
        "encoded_labels[:20], encoded_labels[-20:]\n",
        "#use categorical_crossentropy loss"
      ],
      "metadata": {
        "id": "B0d64nhneJte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORTANT:\n",
        "# stratify: Perform the train-test split with same fraction of labels in training and testing - for imbalanced datasets\n",
        "# Shuffle the data because the labels are ordered!\n",
        "X_train, X_test, y_train, y_test = ....\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "# Show the mapping of original labels to encoded numbers\n",
        "print(\"\\nLabel encoding mapping:\")\n",
        "for i, label_name in enumerate(label_encoder.classes_):\n",
        "    print(f\"{label_name}: {i}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10U4xNYrhTuf",
        "outputId": "750f2b3e-52ba-4c17-a7be-0225517a43f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (2037, 140, 170, 1)\n",
            "X_test shape: (227, 140, 170, 1)\n",
            "y_train shape: (2037, 3)\n",
            "y_test shape: (227, 3)\n",
            "\n",
            "Label encoding mapping:\n",
            "chirp: 0\n",
            "koi_fish: 1\n",
            "low_frequency_burst: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "class PatchExtract(layers.Layer):\n",
        "    \"\"\"Extract patches from images.\"\"\"\n",
        "    def __init__(self, patch_size):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding='VALID'\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n",
        "\n",
        "class MinimalViT(keras.Model):\n",
        "    \"\"\"Minimal Vision Transformer for 140x170 images.\"\"\"\n",
        "    def __init__(self, image_size=(140, 170), patch_size=14, num_classes=3):\n",
        "        super().__init__()\n",
        "        # Adjust to be divisible by patch_size\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (image_size[0] // patch_size) * (image_size[1] // patch_size)\n",
        "        self.embed_dim = 64\n",
        "\n",
        "        # Patch extraction and embedding\n",
        "        self.patch_extract = PatchExtract(patch_size)\n",
        "        self.patch_embed = layers.Dense(self.embed_dim)\n",
        "\n",
        "        # CLS token and position embeddings\n",
        "        self.cls_token = self.add_weight(\n",
        "            shape=(1, 1, self.embed_dim),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='cls_token'\n",
        "        )\n",
        "        self.pos_embed = self.add_weight(\n",
        "            shape=(1, self.num_patches + 1, self.embed_dim),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='pos_embed'\n",
        "        )\n",
        "\n",
        "        # Transformer encoder layers (corrected structure)\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.attn = layers.MultiHeadAttention(num_heads=4, key_dim=self.embed_dim // 4, dropout=0.1)\n",
        "        self.dropout_attn = layers.Dropout(0.1)\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mlp = keras.Sequential([ layers.Dense(self.embed_dim * 2, activation='gelu'), layers.Dropout(0.1), layers.Dense(self.embed_dim), layers.Dropout(0.1), ])\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "\n",
        "        # Extract patches\n",
        "        patches = self.patch_extract(inputs) # (batch, num_patches, patch_sizepatch_size3)\n",
        "\n",
        "        # Embed patches\n",
        "        x = self.patch_embed(patches) # (batch, num_patches, embed_dim)\n",
        "\n",
        "        # Add CLS token\n",
        "        cls_tokens = tf.tile(self.cls_token, [batch_size, 1, 1])\n",
        "        x = tf.concat([cls_tokens, x], axis=1)  # (batch, num_patches+1, embed_dim)\n",
        "\n",
        "        # Add position embeddings\n",
        "        x = x + self.pos_embed\n",
        "        # Single transformer block logic\n",
        "        # Layer Normalization 1\n",
        "        x_norm1 = self.norm1(x)\n",
        "        # Multi-Head Self-Attention\n",
        "        attn_output = self.attn(query=x_norm1, key=x_norm1, value=x_norm1, training=training)\n",
        "        attn_output = self.dropout_attn(attn_output, training=training)\n",
        "        x = x + attn_output # Add skip connection\n",
        "        # Layer Normalization 2 and MLP\n",
        "        x_norm2 = self.norm2(x)\n",
        "        mlp_output = self.mlp(x_norm2, training=training)\n",
        "        x = x + mlp_output # Add skip connection\n",
        "        # Use CLS token for classification\n",
        "\n",
        "\n",
        "        return self.head(x)\n"
      ],
      "metadata": {
        "id": "ZohJmMqzcalZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "num_classes = 3\n",
        "# Assuming your data is already loaded\n",
        "# X_train shape: (n_samples, 140, 170, 3) - now correct with 3 channels\n",
        "# y_train shape: (n_samples, 3) - one-hot encoded\n",
        "\n",
        "# 1. Create minimal ViT model\n",
        "# IMPORTANT: Set num_classes to 3, matching your dataset's actual number of classes\n",
        "num_classes = 3\n",
        "model = MinimalViT(patch_size=14, num_classes=num_classes) # Use the actual num_classes (3)\n",
        "\n",
        "# 2. Build with input shape\n",
        "# Input shape should now be (None, 140, 170, 3) as imgs_processed will have 3 channels\n",
        "model.build(input_shape=(None, 140, 170, 3))\n",
        "\n",
        "# 3. Compile\n",
        "# IMPORTANT: Change loss to 'categorical_crossentropy' because y_train is one-hot encoded\n",
        "model.compile( optimizer='adam', loss='categorical_crossentropy', # Use this for one-hot encoded y_train metrics=['accuracy'] )\n",
        ")\n",
        "\n",
        "# 4. Train\n",
        "history = model.fit( X_train, y_train, validation_split=0.2, epochs=10, batch_size=32 )\n",
        "\n",
        "# 5. Evaluate\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy: {acc:.2%}\")\n",
        "\n",
        "# 6. Predict\n",
        "predictions = model.predict(X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "gKuWxOARcyJM",
        "outputId": "ab08d1eb-cce5-4341-b39f-dd63b711bdee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'MinimalViT' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3324863331.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# IMPORTANT: Set num_classes to 3, matching your dataset's actual number of classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinimalViT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Use the actual num_classes (3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# 2. Build with input shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MinimalViT' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history[\"loss\"])\n",
        "plt.plot(history.history[\"val_loss\"])"
      ],
      "metadata": {
        "id": "qXUYJ3GejRZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1980cfb1"
      },
      "source": [
        "# Convert model predictions from probabilities to class labels\n",
        "y_pred_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Convert one-hot encoded y_test back to original class labels for comparison\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Get class names from the label encoder for better readability in the matrix\n",
        "class_names = label_encoder.classes_\n",
        "\n",
        "print(\"Predicted classes sample:\", y_pred_classes[:10])\n",
        "print(\"True classes sample:\", y_true_classes[:10])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fde24c43"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "usK-m-J2j9Na"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}